---
title: "Class 7: Machine Learning 1"
author: "Sawyer (PID: A16335277)"
format: pdf
---

# Clustering Methods

The broad goal here is to find groupings (clusters) in your input data.

## Kmeans

First, let's make up some data to cluster.

```{r}
x <- rnorm(1000)
hist(x)
```

Make a vector of length 60 with 30 points centered at -3 and 30 points centered at +3
```{r}
tmp <- c(rnorm(30, mean=-3), rnorm(30, mean=3))
```

I will now make a wee x and y dataset with 2 groups of points.

```{r}
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

```{r}
k <- kmeans(x, centers=2)
k
```

> Q. From your result object 'k' how many points are in each cluster?

```{r}
k$size
```

> Q. What "component" of your result object details the cluster membership?

```{r}
k$cluster
```

> Q. Cluster centers?

```{r}
k$centers
```

> Q. Plot of our clustering results

```{r}
plot(x, col=k$cluster)
points(k$centers, col="blue", pch=15, cex=2)
```

Let's cluster with 4 groups

```{r}
k4 <- kmeans(x, centers=4)
plot(x, col=k4$cluster)
points(k4$centers, col="blue", pch=15, cex=2)
```

A big limitation of kmeans is that it does what you ask even if you ask for silly clusters.

## Hierarchical Clustering

The main base R function for Hierarchical Clustering is `hclust()`.
Unlike `kmeans()` you can not just pass it your data as input. You first need to calculate a distance matrix.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

Use `plot()` to view results

```{r}
plot(hc)
abline(h=10, col="red")
```

To make the "cut" and get our cluster membership vector we can use the `cutree()` function.

```{r}
grps <- cutree(hc, h=10)
grps
```

Make a plot of our data colored by hclust results

```{r}
plot(x, col=grps)
```

# Principal Component Analysis (PCA)

Here we will do Principal Component Analysis (PCA) on some food data from the UK.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
```

Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
# we could use nrow(), ncol(), or dim()
dim(x)
```

We have 17 rows and 4 columns

```{r}
## Preview the first 6 rows
head(x)
```

Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

Previously, we had updated the rownames to the first column of the dataframe in-place inside one code cell. This is problematic, because every time we run that particular cell, we will be altering our existing dataframe which is assigned to the variable x. If we update the dataframe with the proper rownames when we load it in, we don't have to worry about this, making it a more robust approach in general.

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

Q3: Changing what optional argument in the above barplot() function results in the following plot?

Aftering calling the help function we can see that the following code produces the desired plot (change beside to FALSE).
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```

This code generates a pairwise scatter plot comparing food consumption between each combination of regions. The points are colored by each food category. If a point lies on the diagonal, that means the consumption metric is identical between the two regions being compared.

Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The blue, orange, and teal data points appear to deviate from the diagonal across all plots comparing Ireland with other regions. This signifies differences in food consumption for these individual points and which foods they correspond to.


## PCA to the rescue

The main "base" R function for PCA is called `prcomp()`.

```{r}
pca <- prcomp(t(x))
summary(pca)
```

> Q. How much variance is captured in 2 PCs

96.5%

To make our main "PC score plot" (a.k.a "PC1 vs PC2 plot", or "PC plot" or "ordination plot").

```{r}
attributes(pca)
```

We are after the `pca$x` result component to make our PCA plot.

Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Plot PC1 vs PC2
mycols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab="PC1 (67.4%)", ylab="PC2 (29%)")
```


Another important result from PCA is how the original variables (in this case the foods) contribute to the PCs.

This is contained in the `pca$rotation` object - folks often call this the "loadings" or "contributions" to the PCs.

```{r}
pca$rotation
```
We can make a plot along PC1.

```{r}
library(ggplot2)

contrib <- as.data.frame(pca$rotation)

ggplot(contrib) +
  aes(PC1, rownames(contrib)) +
  geom_col()
```


